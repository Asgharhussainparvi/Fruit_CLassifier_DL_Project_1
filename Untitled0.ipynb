{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgxgRg5GI-FP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb53340e"
      },
      "source": [
        "# Task\n",
        "Unzip the file \"archive.zip\", delete unnecessary directories, create a YAML file for the dataset, and provide information about the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8edb1f56"
      },
      "source": [
        "## Unzip the archive\n",
        "\n",
        "### Subtask:\n",
        "Unzip the provided `archive.zip` file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbe53baa"
      },
      "source": [
        "**Reasoning**:\n",
        "Unzip the provided `archive.zip` file using the `zipfile` module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50cc9e17"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d78bf05c"
      },
      "source": [
        "## Organize directories\n",
        "\n",
        "### Subtask:\n",
        "Delete any unnecessary directories created during the unzipping process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f084d18a"
      },
      "source": [
        "**Reasoning**:\n",
        "Delete the identified unnecessary directories using the `os` module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0076a565",
        "outputId": "31a991f9-40b3-480c-884c-fe83dae3da52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory not found: __MACOSX\n",
            "\n",
            "Contents of the current directory after removal:\n",
            "['.config', 'archive.zip', 'Ripe & Unripe Fruits', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Identify directories to remove. This is based on the expected contents of the archive.\n",
        "directories_to_remove = ['__MACOSX'] # Example: if __MACOSX is created by the unzip process\n",
        "\n",
        "for directory in directories_to_remove:\n",
        "    if os.path.exists(directory):\n",
        "        shutil.rmtree(directory)\n",
        "        print(f\"Removed directory: {directory}\")\n",
        "    else:\n",
        "        print(f\"Directory not found: {directory}\")\n",
        "\n",
        "# Verify that the directories have been removed\n",
        "print(\"\\nContents of the current directory after removal:\")\n",
        "print(os.listdir('.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1850ec6"
      },
      "source": [
        "## Create YAML file\n",
        "\n",
        "### Subtask:\n",
        "Generate a YAML file containing the dataset configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8abe8651"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a dictionary with the dataset information and save it as a YAML file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffe6b054",
        "outputId": "7ca04a9b-c8cd-4f2d-bfc6-a1865c0edbe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset_config.yaml created successfully.\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Define the base path where the dataset is located\n",
        "dataset_base_path = '/content/Ripe & Unripe Fruits'\n",
        "\n",
        "# Define the class names based on the 11 ripe and 11 unripe categories\n",
        "# You might need to adjust these names to match the actual directory names if they are different\n",
        "class_names = [\n",
        "    'ripe apple', 'ripe banana', 'ripe dragon', 'ripe grapes', 'ripe lemon', 'ripe mango',\n",
        "    'ripe orange', 'ripe papaya', 'ripe pineapple', 'ripe pomegranate', 'ripe strawberry',\n",
        "    'unripe apple', 'unripe banana', 'unripe dragon', 'unripe grapes', 'unripe lemon',\n",
        "    'unripe mango', 'unripe orange', 'unripe papaya', 'unripe pineapple', 'unripe pomegranate',\n",
        "    'unripe strawberry'\n",
        "]\n",
        "\n",
        "dataset_config = {\n",
        "    'path': dataset_base_path,  # Base path to the dataset\n",
        "    'train': dataset_base_path, # Assuming training data is directly in the base path or subdirectories within it\n",
        "    'val': dataset_base_path,   # Assuming validation data is structured similarly\n",
        "    'nc': len(class_names), # Number of classes\n",
        "    'names': class_names # Class names\n",
        "}\n",
        "\n",
        "with open('dataset_config.yaml', 'w') as file:\n",
        "    yaml.dump(dataset_config, file, default_flow_style=False)\n",
        "\n",
        "print(\"dataset_config.yaml created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ad8571"
      },
      "source": [
        "## Dataset Information\n",
        "\n",
        "### Subtask:\n",
        "Provide information about the dataset, including the number of classes and examples per class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b566595"
      },
      "source": [
        "**Reasoning**:\n",
        "Read the dataset configuration from the generated YAML file, list the class names, count the total number of classes, and iterate through the dataset directory to count the number of examples in each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0699499",
        "outputId": "daab9d69-4945-45a0-df1f-479485038fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Names: ['ripe apple', 'ripe banana', 'ripe dragon', 'ripe grapes', 'ripe lemon', 'ripe mango', 'ripe orange', 'ripe papaya', 'ripe pineapple', 'ripe pomegranate', 'ripe strawberry', 'unripe apple', 'unripe banana', 'unripe dragon', 'unripe grapes', 'unripe lemon', 'unripe mango', 'unripe orange', 'unripe papaya', 'unripe pineapple', 'unripe pomegranate', 'unripe strawberry']\n",
            "\n",
            "Total Number of Classes: 22\n",
            "\n",
            "Examples per Class in Training Dataset:\n",
            "ripe apple: 388 examples\n",
            "ripe banana: 400 examples\n",
            "ripe dragon: 399 examples\n",
            "ripe grapes: 398 examples\n",
            "ripe lemon: 386 examples\n",
            "ripe mango: 399 examples\n",
            "ripe orange: 400 examples\n",
            "ripe papaya: 400 examples\n",
            "ripe pineapple: 400 examples\n",
            "ripe pomegranate: 390 examples\n",
            "ripe strawberry: 400 examples\n",
            "unripe apple: 391 examples\n",
            "unripe banana: 400 examples\n",
            "unripe dragon: 400 examples\n",
            "unripe grapes: 397 examples\n",
            "unripe lemon: 396 examples\n",
            "unripe mango: 400 examples\n",
            "unripe orange: 400 examples\n",
            "unripe papaya: 400 examples\n",
            "unripe pineapple: 380 examples\n",
            "unripe pomegranate: 389 examples\n",
            "unripe strawberry: 400 examples\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Load the dataset configuration from the YAML file\n",
        "with open('dataset_config.yaml', 'r') as file:\n",
        "    dataset_config = yaml.safe_load(file)\n",
        "\n",
        "# 1. List the names of the classes\n",
        "class_names = dataset_config['names']\n",
        "print(\"Class Names:\", class_names)\n",
        "\n",
        "# 2. Count the number of classes\n",
        "num_classes = dataset_config['nc']\n",
        "print(\"\\nTotal Number of Classes:\", num_classes)\n",
        "\n",
        "# 3. Iterate through the training directory and count examples per class\n",
        "train_dir = dataset_config['train']\n",
        "examples_per_class = {}\n",
        "\n",
        "print(\"\\nExamples per Class in Training Dataset:\")\n",
        "if os.path.isdir(train_dir):\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(train_dir, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            # Count image files (assuming they are jpg, jpeg, or png)\n",
        "            num_examples = len([f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            examples_per_class[class_name] = num_examples\n",
        "            # 5. Print the name of each class along with the number of examples\n",
        "            print(f\"{class_name}: {num_examples} examples\")\n",
        "        else:\n",
        "            print(f\"Warning: Directory not found for class '{class_name}': {class_dir}\")\n",
        "else:\n",
        "    print(f\"Error: Training directory not found at {train_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWYM16Q9M1Rg"
      },
      "source": [
        "## Train a Classification Model\n",
        "\n",
        "### Subtask:\n",
        "Train a classification model (e.g., ResNet) on the \"Ripe & Unripe Fruits\" dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDyqjALyMvkw",
        "outputId": "5e66648e-a0f1-41fc-9b82-f36ae759bc8f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 129MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/9\n",
            "----------\n",
            "train Loss: 2.5279 Acc: 0.2614\n",
            "val Loss: 2.0220 Acc: 0.4772\n",
            "Epoch 1/9\n",
            "----------\n",
            "train Loss: 2.1905 Acc: 0.3502\n",
            "val Loss: 1.9543 Acc: 0.5198\n",
            "Epoch 2/9\n",
            "----------\n",
            "train Loss: 2.0245 Acc: 0.3949\n",
            "val Loss: 1.5830 Acc: 0.5757\n",
            "Epoch 3/9\n",
            "----------\n",
            "train Loss: 1.9037 Acc: 0.4290\n",
            "val Loss: 1.3730 Acc: 0.6063\n",
            "Epoch 4/9\n",
            "----------\n",
            "train Loss: 1.8537 Acc: 0.4438\n",
            "val Loss: 1.3844 Acc: 0.6403\n",
            "Epoch 5/9\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "import os\n",
        "\n",
        "# Define data transformations (adjust as needed)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Define the data directory (using the path from the dataset_config)\n",
        "data_dir = dataset_config['path'] # Use the path from the dataset_config\n",
        "\n",
        "# Create data loaders\n",
        "# Fix: Removed os.path.join(data_dir, x) because class directories are directly in data_dir\n",
        "image_datasets = {x: datasets.ImageFolder(data_dir, data_transforms[x])\n",
        "                  for x in ['train', 'val']} # Assuming 'train' and 'val' phases, but using the same data_dir for simplicity here. You might need to split data into train/val folders for a proper split.\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'val']} # Adjust batch size and num_workers as needed\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load a pre-trained ResNet model\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Modify the final layer to match the number of classes\n",
        "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# --- Training Loop (Basic Example) ---\n",
        "# You would typically add more to this, like a learning rate scheduler,\n",
        "# saving checkpoints, and evaluating on the validation set.\n",
        "\n",
        "num_epochs = 10 # Define the number of epochs to train for\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model_ft.train()  # Set model to training mode\n",
        "        else:\n",
        "            model_ft.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Iterate over data.\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer_ft.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model_ft(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer_ft.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "print('\\nTraining complete!')\n",
        "\n",
        "# You would typically save the trained model here\n",
        "# torch.save(model_ft.state_dict(), 'fruit_classifier_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qb_yjOdM3OV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}